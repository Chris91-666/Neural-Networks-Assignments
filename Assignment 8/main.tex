\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{listings}

%nicer tables
\usepackage{booktabs}

\usepackage{graphicx}

\usepackage{float}

\usepackage{color}
\usepackage{url}
\usepackage{hyperref}

\usepackage{enumerate}

\usepackage[backend=biber]{biblatex}

\usepackage{csquotes}

\usepackage{multicol}
\setlength{\columnsep}{1cm}
\setlength{\headheight}{36pt}

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}



%
\usepackage{amsmath, amsthm, amssymb}
\usepackage[ngerman, english]{babel}
\usepackage{marvosym}
\usepackage{graphics}
\usepackage{extarrows}
\usepackage{forloop}
\usepackage{mathtools}

\usepackage[]{algorithm2e}

\usepackage{hyperref}% http://ctan.org/pkg/hyperref
\usepackage{cleveref}% http://ctan.org/pkg/cleveref
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{preliminary}{Preliminary}
\newtheorem{notation}{Notation}
\newtheorem{property}{Property}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{hypothesis}{Hypothesis}

\crefname{theorem}{Theorem}{Theorems}
\crefname{definition}{Definition}{Definitions}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{preliminary}{Preliminary}{Preliminaries}
\crefname{notation}{Notation}{Notations}
\crefname{property}{Property}{Properties}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{example}{Example}{Examples}
\crefname{hypothesis}{Hypothesis}{Hypotheses}

\newenvironment{beweis}{\begin{proof}[Beweis]}{\end{proof}}
%



\lstset{language=Python,
showspaces=false,
showtabs=false,
breaklines=true,
showstringspaces=false,
breakatwhitespace=true,
escapeinside={(*@}{@*)},
commentstyle=\color{greencomments},
keywordstyle=\color{bluekeywords}\bfseries,
stringstyle=\color{redstrings},
basicstyle=\ttfamily
}


\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\frenchspacing
\pagestyle{fancy}
\sloppy 

\markright{headline}

\addbibresource{references.bib}

\begin{document}

\lhead{\begin{tabular}{l}
\\
Neural Networks\\
WiSe 2020/2021\\
\end{tabular}}

\rhead{\begin{tabular}{r}
Assignment 8\\
Simon Laurent Lebailly, 2549365, s9sileba@teams.uni-saarland.de\\%% <=== Also HERE if you have a team mateUpdate Name HERE !!! 
Christian Mathieu Schmidt, 2537621, s9cmscmi@teams.uni-saarland.de
\end{tabular}}




\section*{Exercise 8.1: Empirical Risk Minimization}
    \subsection*{a)}
        

    \subsection*{b)}
        
    
    \subsection*{c)}
        



\newpage
\section*{Exercise 8.2: First-Order Optimization}
    \subsection*{a)}
        

    
    \subsection*{b)}
        


    \subsection*{c)}
        



\newpage
\section*{Exercise 8.3: Second-Order Optimization}
    \subsection*{a)}
        First-Order optimization methods only look for the gradient to find the local minimum.
        Second-Order optimization methods also find a downhill direction, but they consider not only the gradient, they also consider the curvature.
        Thus, if the algorithm comes closer to a local maximum, the Hessian will be positive definite.
        If the algorithm comes closer to a local minimum, the Hessian will be negative definite.
        And if the algorithm comes closer to a saddle point, the Hessian will have positive and negative eigenvalues.\\
        In short, unlike first-order optimization methods, second-order optimization methods do not ignore the curvature of a surface and thus improve the result in each individual optimization step.\\
        
        Since we derive twice it is of course necessary that the function is twice differentiable.
        

    \subsection*{b)}
        Definition of Taylor series and Newton's method from Deep Learning book by Ian Goodfellow (page 307).
        Approximation of $J(\Theta)$ near some point $\Theta_0$ with second-order Taylor series, ignoring derivatives of higher order:
        \begin{align}
            J(\Theta) &\approx J(\Theta_0) + (\Theta - \Theta_0)^T \nabla_{\Theta} J(\Theta_0) + \frac{1}{2} (\Theta - \Theta_0)^T H(\Theta - \Theta_0)
        \end{align}
        where $H$ is the Hessian of $J$ with respect to $\Theta$ evaluated at $\Theta_0$.\\
        The Newton parameter update rule:
        \begin{align}
            \Theta^* &= \Theta_0 - H^{-1} \nabla_{\Theta} J(\Theta_0)
        \end{align}
        \textbf{Usability of Newton Method:}\\
        With the Newton method one tries to approach the minimum of a function by approaching the zero of the derivative, which belongs to this minimum.
        Thereby the distance of the zero point of the tangent of the derivative at the momentarily considered point is approached more and more to the zero point of the derivative.
        This intersection is then considered as a new point, which is why the Newton method is an iterative method.
        The distance of the intersection point of the tangent of the derivative at the momentarily considered point to the zero of the derivative decreases thereby quadratically.
        Thus Newton's method allows a fast approach to the zero of the first derivative by considering the first and second derivative.

    
    \subsection*{c)}
        


    \subsection*{d)}
        



\end{document}
