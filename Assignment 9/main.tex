\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{listings}

%nicer tables
\usepackage{booktabs}

\usepackage{graphicx}

\usepackage{float}

\usepackage{color}
\usepackage{url}
\usepackage{hyperref}

\usepackage{enumerate}

\usepackage[backend=biber]{biblatex}

\usepackage{csquotes}

\usepackage{multicol}
\setlength{\columnsep}{1cm}
\setlength{\headheight}{36pt}

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}



%
\usepackage{amsmath, amsthm, amssymb}
\usepackage[ngerman, english]{babel}
\usepackage{marvosym}
\usepackage{graphics}
\usepackage{extarrows}
\usepackage{forloop}
\usepackage{mathtools}

\usepackage[]{algorithm2e}

\usepackage{hyperref}% http://ctan.org/pkg/hyperref
\usepackage{cleveref}% http://ctan.org/pkg/cleveref
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{preliminary}{Preliminary}
\newtheorem{notation}{Notation}
\newtheorem{property}{Property}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{hypothesis}{Hypothesis}

\crefname{theorem}{Theorem}{Theorems}
\crefname{definition}{Definition}{Definitions}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{preliminary}{Preliminary}{Preliminaries}
\crefname{notation}{Notation}{Notations}
\crefname{property}{Property}{Properties}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{example}{Example}{Examples}
\crefname{hypothesis}{Hypothesis}{Hypotheses}

\newenvironment{beweis}{\begin{proof}[Beweis]}{\end{proof}}
%



\lstset{language=Python,
showspaces=false,
showtabs=false,
breaklines=true,
showstringspaces=false,
breakatwhitespace=true,
escapeinside={(*@}{@*)},
commentstyle=\color{greencomments},
keywordstyle=\color{bluekeywords}\bfseries,
stringstyle=\color{redstrings},
basicstyle=\ttfamily
}


\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\frenchspacing
\pagestyle{fancy}
\sloppy 

\markright{headline}

\addbibresource{references.bib}

\begin{document}

\lhead{\begin{tabular}{l}
\\
Neural Networks\\
WiSe 2020/2021\\
\end{tabular}}

\rhead{\begin{tabular}{r}
Assignment 9\\
Simon Laurent Lebailly, 2549365, s9sileba@teams.uni-saarland.de\\%% <=== Also HERE if you have a team mateUpdate Name HERE !!! 
Christian Mathieu Schmidt, 2537621, s9cmscmi@teams.uni-saarland.de
\end{tabular}}




\section*{Exercise 9.1: Convolutions}
    \subsection*{a)}
        Let $f(x) = e^{-x}$ and $g(x) = sin(x)$.
        Then we can compute the convolution $f(x) \ast g(x)$ as follows
        \begin{align}
            f(x) \ast g(x) &= (f \ast g)(x) = (g \ast f)(x)\\
            &= \int\limits_{t=0}^x f(x-t) g(t) dt\\
            &= \int\limits_{t=0}^x e^{-x+t} sin(t) dt\\
            &= \int\limits_{t=0}^x e^{-x} e^{t} sin(t) dt\\
            &= e^{-x} \cdot \int\limits_{t=0}^x e^{t} sin(t) dt
        \end{align}
        To solve the integral we use \textbf{two times partial integration:} $\int m'n = mn - \int mn'$\\
        \textbf{First time:} We set $m(t) = e^t$, $n(t) = sin(t)$, and become $m'(t) = e^t$, $n'(t) = cos(t)$
        \begin{align}
            \int\limits_{t=0}^x e^{t} sin(t) dt &= \left[ e^{t} sin(t) \right]_{t=0}^x - \int\limits_{t=0}^x e^{t} cos(t) dt
        \end{align}
        \textbf{Second time:} We set $m(t) = e^t$, $n(t) = cos(t)$, and become $m'(t) = e^t$, $n'(t) = -sin(t)$
        \begin{align}
            \int\limits_{t=0}^x e^{t} cos(t) dt &= \left[ e^{t} cos(t) \right]_{t=0}^x - \int\limits_{t=0}^x - e^{t} sin(t) dt\\
            &= \left[ e^{t} cos(t) \right]_{t=0}^x + \int\limits_{t=0}^x e^{t} sin(t) dt
        \end{align}
        Now we put the result of the two partial integrations together, and get the following equation
        \begin{align}
            & & \int\limits_{t=0}^x e^{t} sin(t) dt &= \left[ e^{t} sin(t) \right]_{t=0}^x - \left( \left[ e^{t} cos(t) \right]_{t=0}^x + \int\limits_{t=0}^x e^{t} sin(t) dt \right)\\
            & & &= \left[ e^{t} \left( sin(t) - cos(t) \right) \right]_{t=0}^x - \int\limits_{t=0}^x e^{t} sin(t) dt\\
            &\Leftrightarrow & 2 \cdot \int\limits_{t=0}^x e^{t} sin(t) dt &= \left[ e^{t} \left( sin(t) - cos(t) \right) \right]_{t=0}^x\\
            &\Leftrightarrow & \int\limits_{t=0}^x e^{t} sin(t) dt &= \frac{1}{2} \left[ e^{t} \left( sin(t) - cos(t) \right) \right]_{t=0}^x
            = \frac{1}{2} \left( e^{x} \left( sin(x) - cos(x) \right) + 1 \right)
        \end{align}
        If we now put all together, we yield
        \begin{align}
            f(x) \ast g(x) &= (f \ast g)(x) = (g \ast f)(x)\\
            &= e^{-x} \cdot \frac{1}{2} \left( e^{x} \left( sin(x) - cos(x) \right) + 1 \right)
            = \frac{1}{2} \left( sin(x) - cos(x) + e^{-x} \right)
        \end{align}
        

    \subsection*{b)}
        Let $f(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}$ and $F^2(x) = (f \ast f)(x)$.
        \textbf{To show:} $F^{\infty}(x) = 0$
        \begin{proof}
            We know that $f(x) = \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}$ is the density function of the gaussian distribution 
            $\mathcal{N}(\mu,\,\sigma^{2})$ with mean $\mu = 0$ and standard deviation $\sigma^2 =1$, 
            also called density function of the "standard normal distribution" $\mathcal{N}(0,1)$.\\
            For the density function of the standard normal distribution it holds
            \begin{align}
                \int\limits_{-\infty}^{\infty} f(x) dx 
                &= \int\limits_{-\infty}^{\infty} \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} dx = 1
            \end{align}
            So the convolution $F^{\infty}(x)$ can be computed as follows:
            \begin{align}
                F^{\infty}(x) &= \lim_{n \rightarrow \infty} F^n(x)\\
                &= \lim_{n \rightarrow \infty} \left( \frac{1}{\sqrt{2 \pi}} \right)^n \cdot G^n(x)
            \end{align}
            with $G^n(x) = (\underbrace{g \ast ... \ast g}_{n\text{ times}})(x)$ and $g(x) = e^{-\frac{x^2}{2}}$, we can assume
            $0 < g(x) < 1$ for all $x /in \mathbb{R}\backslash\{0\}$,\\ and equal one for $x=0$.\\
            We can follow that $0 < G^{n+1}(x) < G^{n} < \int\limits_{-\infty}^{\infty} g(x) dx = \sqrt{2\pi}$ for any $n \in \mathbb{N}$, and thus
            \begin{align}
                G^{\infty}(x) &= \lim_{n \rightarrow \infty} G^n(x) = 0
            \end{align}
            And so we can conclude
            \begin{align}
                F^{\infty}(x) &= \lim_{n \rightarrow \infty} \left( \frac{1}{\sqrt{2 \pi}} \right)^n \cdot G^n(x)\\
                &= \underbrace{\lim_{n \rightarrow \infty} \left( \frac{1}{\sqrt{2 \pi}} \right)^n}_{=0} \cdot \underbrace{\lim_{n \rightarrow \infty} G^n(x)}_{=0\ (18)}
                = 0 \cdot 0 = 0
            \end{align}
            
        \end{proof}

\newpage
    \subsection*{c)}
        \begin{itemize}
            \item \textbf{Dimension output feature map:}\\
                \begin{align}
                    n_{out} &= \left\lfloor \frac{n_{in} + 2*p - k}{s} \right\rfloor + 1
                    = \left\lfloor \frac{100 + 2*0 - 7}{2} \right\rfloor + 1\\
                    &= \left\lfloor \frac{93}{2} \right\rfloor + 1
                    = \left\lfloor 46,5 \right\rfloor + 1 = 46 + 1 = 47
                \end{align}
                with $n_{in}$ as number of input features, $n_{out}$ as number of output features, $k$ as convolution kernel size, 
                $p$ as convolution padding size, and $s$ as convolution stride size.\\
                Therefore, the output feature map has the following dimension:
                $$47 \times 47 \times 64$$
            
            \item \textbf{Total number of weight parameters $n_w$:}\\
                \begin{align}
                    n_w &= \underbrace{(7 * 7)}_{\text{Weights from the kernel}} *\ \ \ \ \ \ \ \ \ \  (\underbrace{32}_{\text{Input channels}} * \underbrace{2}_{\text{Filters}})\\ 
                    &= 49\ \ \ \ \ \ \ \  * \underbrace{64}_{\text{Output channels}}\\ 
                    &= 3136
                \end{align}

        \end{itemize}

    \subsection*{d)}
        \subsubsection*{i)}
            

        \subsubsection*{i)}
            



\newpage
\section*{Exercise 9.2: Batch Normalization}
    




\newpage
\section*{Exercise 9.3: CNN}
    see Jupyter Notebook file!



\end{document}
