\documentclass[a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{vmargin}
\usepackage{listings}

%nicer tables
\usepackage{booktabs}

\usepackage{graphicx}

\usepackage{float}

\usepackage{color}
\usepackage{url}
\usepackage{hyperref}

\usepackage{enumerate}

\usepackage[backend=biber]{biblatex}

\usepackage{multicol}
\setlength{\columnsep}{1cm}
\setlength{\headheight}{36pt}

\definecolor{bluekeywords}{rgb}{0.13,0.13,1}
\definecolor{greencomments}{rgb}{0,0.5,0}
\definecolor{redstrings}{rgb}{0.9,0,0}



%
\usepackage{amsmath, amsthm, amssymb}
\usepackage[ngerman, english]{babel}
\usepackage{marvosym}
\usepackage{graphics}
\usepackage{extarrows}
\usepackage{forloop}
\usepackage{mathtools}

\usepackage[]{algorithm2e}

\usepackage{hyperref}% http://ctan.org/pkg/hyperref
\usepackage{cleveref}% http://ctan.org/pkg/cleveref
\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{preliminary}{Preliminary}
\newtheorem{notation}{Notation}
\newtheorem{property}{Property}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newtheorem{hypothesis}{Hypothesis}

\crefname{theorem}{Theorem}{Theorems}
\crefname{definition}{Definition}{Definitions}
\crefname{lemma}{Lemma}{Lemmas}
\crefname{preliminary}{Preliminary}{Preliminaries}
\crefname{notation}{Notation}{Notations}
\crefname{property}{Property}{Properties}
\crefname{corollary}{Corollary}{Corollaries}
\crefname{example}{Example}{Examples}
\crefname{hypothesis}{Hypothesis}{Hypotheses}

\newenvironment{beweis}{\begin{proof}[Beweis]}{\end{proof}}
%



\lstset{language=Python,
showspaces=false,
showtabs=false,
breaklines=true,
showstringspaces=false,
breakatwhitespace=true,
escapeinside={(*@}{@*)},
commentstyle=\color{greencomments},
keywordstyle=\color{bluekeywords}\bfseries,
stringstyle=\color{redstrings},
basicstyle=\ttfamily
}


\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt}

\frenchspacing
\pagestyle{fancy}
\sloppy 

\markright{headline}

\addbibresource{references.bib}

\begin{document}

\lhead{\begin{tabular}{l}
\\
Neural Networks\\
Saarland University WiSe 2020/2021\\
\end{tabular}}

\rhead{\begin{tabular}{r}
Assignment 5\\
Simon Laurent Lebailly, 2549365\\%% <=== Also HERE if you have a team mateUpdate Name HERE !!! 
Christian Schmidt, 2537621
\end{tabular}}




\section*{Exercise 5.1 - Gradient Descent}
$(1+0,5+1 = 2,5 \text{ points})$
    \subsection*{a)}
        
        
        
    \subsection*{b)}
        \begin{align}
            L_i = \sum\limits_{j=1}^k ||x_i -\mu_j||^2
        \end{align}
        If we derive $L_i$ with respect to $\mu_1$ we become
        $$L'_i = -2 \cdot ||x_i - \mu_1||$$
        
        
    \subsection*{c)}
        
    
    
        


\newpage
\section*{Exercise 5.2 - Weight Space Symmetry}
$(1+0,5 = 1,5 \text{ points})$
    \subsection*{a)}
    When applying the change of signs on any neuron for the input and output simultaneously, we can come up with exactly one equivalent transformation. Therefore, for M neurons, we can derive M equivalent transformations.\\
    Similarly, combining each neuron with every other neuron yields a total of $\sum_{i=1}^{M-1} i$ equivalent transformations when interchanging the weights of their inputs and outputs simultaneously. However, since we can apply this to both the original neurons and their equivalent transformations based on the change of signs, this yields a total of $\sum_{i=1}^{2M-1} i$ equivalent transformations.
    
    \subsection*{b)}
    Each layer can have $\sum_{j=1}^{2M_i-1}j$ transformations, meaning the total number of equivalent transformations for the whole network is $\sum_{i=1}^{N}\sum_{j=1}^{2M_i-1}j$.
        



\newpage
\section*{Exercise 5.3 - Hessian and Optimization}
$(1+1 = 2 \text{ points})$
    \subsection*{\underline{Figure (a):}}
        \includegraphics[width=0.8\linewidth]{Assignment 5/1.png}
        \subsubsection*{a)}
            As we can see, $x'$ is a local extremum of $f(x)$. To be more precise, $x'$ seems to be a local minimum.
            And from the view of the representative red arrows, the function $f(x)$ is increasing from $x'$ in all directions.
            Hence the surface of the function is concave up from the view of point $x'$.
            As a function that maps from two dimensional space, the Hessian matrix has exactly two eigenvectors, and therefore exactly two eigenvalues.
            Looking at the slope, the function is increasing in all directions from $x'$ as just mentioned.
            As we know from the exercise description, the curvature in the direction of an eigenvector is determined by its eigenvalue.\\
            Thus, all eigenvalues have a positive sign.

        \subsubsection*{b)}
            Since all eigenvalues of the symmetric Hessian matrix $A$ are greater than zero, we can conclude that $X^T A X$ is greater than zero, and thus $A$ is positive definite per definition.


\newpage
    \subsection*{\underline{Figure (b):}}
        \includegraphics[width=0.8\linewidth]{Assignment 5/2.png}
        \subsubsection*{a)}
            As we can see, $x'$ is a local extremum of $f(x)$. To be more precise, $x'$ seems to be a local maximum.
            And from the view of the representative red arrows, the function $f(x)$ is decreasing from $x'$ in all directions.
            Hence the surface of the function is concave down from the view of point $x'$.
            As a function that maps from two dimensional space, the Hessian matrix has exactly two eigenvectors, and therefore exactly two eigenvalues.
            Looking at the slope, the function is decreasing in all directions from $x'$ as just mentioned.
            As we know from the exercise description, the curvature in the direction of an eigenvector is determined by its eigenvalue.\\
            Thus, all eigenvalues have a negative sign.

        \subsubsection*{b)}
            Since all eigenvalues of the symmetric Hessian matrix $A$ are less than zero, we can conclude that $X^T A X$ is less than zero, and thus $A$ is negative definite per definition.


\newpage
    \subsection*{\underline{Figure (c):}}
        \includegraphics[width=0.8\linewidth]{Assignment 5/3.png}
        \subsubsection*{a)}
            $g$ represents a line, which has either a low or no gradient, and on this figure raises the question of a local minimum.
            Intuitively, we would assume from the problem that $g$ has no gradient. 
            But since we have to make this decision on the basis of a plot with poor resolution, we treat both cases for completeness!
            If $g$ has no gradient, there are an infinite number of extreme points (minimums), which all lie on the straight line $g$, and if $g$ has no gradient, there is not a single minimum.
            In both cases, however, there is no curvature in the direction of the straight line $g$.
            Only in the direction of the red arrows, i.e. orthogonal to $g$ in both directions, the surface is curved upwards.
            As a function that maps from two dimensional space, the Hessian matrix has exactly two eigenvectors, and therefore exactly two eigenvalues.
            Looking at the slope, the function only increases in the directions which are orthogonal to the direction of $g$.
            As we know from the exercise description, the curvature in the direction of an eigenvector is determined by its eigenvalue.\\
            Thus, one eigenvalue has a positive sign, the other is zero.

        \subsubsection*{b)}
            Since all eigenvalues of the symmetric Hessian matrix $A$ are greater or equal than zero, we can conclude that $X^T A X$ is greater or equal than zero, and thus $A$ is positive semidefinite per definition.
            But as required in the task, we have to decide if $A$ is positive definite, negative definite or neither.
            And since $A$ can not be positive or negative definite, the Hessian matrix is indefinite.


\newpage
    \subsection*{\underline{Figure (d):}}
        \includegraphics[width=0.8\linewidth]{Assignment 5/4.png}
        \subsubsection*{a)}
            As we can see, $x'$ is a local extremum of $f(x)$. To be more precise, $x'$ seems to be a saddle point.
            And from the view of the representative arrows, the function $f(x)$ is increasing from $x'$ in two opposite directions (green arrows), and decreases in the two orthogonal directions (red arrows).
            Hence the surface of the function is concave up and concave down from the view of point $x'$.
            As a function that maps from two dimensional space, the Hessian matrix has exactly two eigenvectors, and therefore exactly two eigenvalues.
            As we know from the exercise description, the curvature in the direction of an eigenvector is determined by its eigenvalue.\\
            Thus, one eigenvalue has a positive sign, the other has a negative sign.

        \subsubsection*{b)}
            Since one eigenvalue of the symmetric Hessian matrix $A$ is greater than zero, and the other is less than zero, we can conclude that $X^T A X$, for a definite matrix $A$, has to be greater and less than zero at the same time.
            Since this is not possible, $A$ is an indefinite matrix.
            


\newpage
\section*{Exercise 5.4 - Gradient Descent and Newton's Method}
$(1+1+0,5 = 2,5 \text{ points})$
    \subsection*{a)}
        First we have to compute the gradient:
        \begin{align}
            \nabla f(x) = \left( \begin{matrix} 2x_1 - x_2 -3 \\ 2x_2 - x_1 \end{matrix} \right)
        \end{align}
        First we will compute the L2-norm of $x^{(0)}$:
            \begin{align*}
                ||\nabla f(x^{(0)})||_2 = \sqrt{2^2 + 1^2} = \sqrt{5} \approx 2,236 > 0,2
            \end{align*}
            The L2-norm is greater than $0,2$ $\Rightarrow$ Go to iteration 1.

        \subsubsection*{1. Iteration:}
            \begin{align*}
                x^{(1)} &= x^{(0)} - \epsilon \cdot \nabla f(x^{(0)})\\
                &= \left( \begin{matrix} 1 \\ 1 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} 2-1-3 \\ 2-1 \end{matrix} \right)\\
                &= \left( \begin{matrix} 1 \\ 1 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} -2 \\ 1 \end{matrix} \right)
                = \left( \begin{matrix} 2 \\ 0,5 \end{matrix} \right)
            \end{align*}
            Now we will compute the L2-norm of $x^{(1)}$:
            \begin{align*}
                ||\nabla f(x^{(1)})||_2 = \sqrt{0,25 + 1} \approx 1,118 > 0,2
            \end{align*}
            The L2-norm is greater than $0,2$ $\Rightarrow$ Go to iteration 2.
            
        \subsubsection*{2. Iteration:}
            \begin{align*}
                x^{(2)} &= x^{(1)} - \epsilon \cdot \nabla f(x^{(1)})\\
                &= \left( \begin{matrix} 2 \\ 0,5 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} 4-0,5-3 \\ 1-2 \end{matrix} \right)\\
                &= \left( \begin{matrix} 2 \\ 0,5 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} 0,25 \\ -0,5 \end{matrix} \right)
                = \left( \begin{matrix} 1,75 \\ 1 \end{matrix} \right)
            \end{align*}
            Now we will compute the L2-norm of $x^{(2)}$:
            \begin{align*}
                ||\nabla f(x^{(2)})||_2 = \frac{1}{4} \sqrt{5} \approx 0,559 > 0,2
            \end{align*}
            The L2-norm is greater than $0,2$ $\Rightarrow$ Go to iteration 3.

        \subsubsection*{3. Iteration:}
            \begin{align*}
                x^{(3)} &= x^{(2)} - \epsilon \cdot \nabla f(x^{(2)})\\
                &= \left( \begin{matrix} 1,75 \\ 1 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} 2*1,75-1-3 \\ 2-1,75 \end{matrix} \right)\\
                &= \left( \begin{matrix} 1,75 \\ 1 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} -0,5 \\ 0,25 \end{matrix} \right)
                = \left( \begin{matrix} 2 \\ 0,875 \end{matrix} \right)
            \end{align*}
            Now we will compute the L2-norm of $x^{(3)}$:
            \begin{align*}
                ||\nabla f(x^{(3)})||_2 = \frac{1}{8} \sqrt{5} \approx 0,280 > 0,2
            \end{align*}
            The L2-norm is greater than $0,2$ $\Rightarrow$ Go to iteration 4.

        \subsubsection*{4. Iteration:}
            \begin{align*}
                x^{(4)} &= x^{(3)} - \epsilon \cdot \nabla f(x^{(3)})\\
                &= \left( \begin{matrix} 2 \\ 0,875 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} 4-0,875-3 \\ 1,75-2 \end{matrix} \right)\\
                &= \left( \begin{matrix} 2 \\ 0,875 \end{matrix} \right) - 0,5 \cdot \left( \begin{matrix} 0,125 \\ -0,25 \end{matrix} \right)
                = \left( \begin{matrix} 1,9375 \\ 1 \end{matrix} \right)
            \end{align*}
            Now we will compute the L2-norm of $x^{(4)}$:
            \begin{align*}
                ||\nabla f(x^{(4)})||_2 = \frac{1}{16} \sqrt{5} \approx 0,140 < 0,2
            \end{align*}
            The L2-norm is less than $0,2$ $\Rightarrow$ Stop in step 4 by point $x^{(4)}$!
            
        
    \subsection*{b)}
        First we have to compute the Jacobian matrix:
        \begin{align}
            J_f(x) = \nabla f(x) = \left( \begin{matrix} 2x_1 - x_2 -3 \\ 2x_2 - x_1 \end{matrix} \right)
        \end{align}
        Now we will derive the Hessian matrix:
        \begin{align}
            H_f(x) = \left( \begin{matrix} 2 & -1 \\ -1 & 2 \end{matrix} \right)
        \end{align}
        The inverse of the Hessian matrix:
        \begin{align}
            H_f(x)^{-1} = \frac{1}{3} \cdot \left( \begin{matrix} 2 & 1 \\ 1 & 2 \end{matrix} \right)
        \end{align}
        The formula for the Newton method is as follow:
        \begin{align}
            x = x^{(0)} - H_f(x^{(0)})^{-1} J_f(x^{(0)})
        \end{align}
        Now we will compute the extremum $x'$ of $f$:
        \begin{align}
            x' &= x^{(0)} - H_f(x^{(0)})^{-1} J_f(x^{(0)})\\
            &= \left( \begin{matrix} 1 \\ 1 \end{matrix} \right) - \frac{1}{3} \cdot \left( \begin{matrix} 2 & 1 \\ 1 & 2 \end{matrix} \right) \cdot \left( \begin{matrix} -2 \\ 1 \end{matrix} \right)\\
            &= \left( \begin{matrix} 1 \\ 1 \end{matrix} \right) - \frac{1}{3} \cdot \left( \begin{matrix} -3 \\ 0 \end{matrix} \right)
            = \left( \begin{matrix} 2 \\ 1 \end{matrix} \right)
        \end{align}
        Test:
        \begin{align}
            \nabla f(x') = \left( \begin{matrix} 2 \cdot 2 - 1 - 3 \\ 2 \cdot 1 -2 \end{matrix} \right) = \left( \begin{matrix} 0 \\ 0 \end{matrix} \right)
        \end{align}
        $\Rightarrow$ $x'$ is an extremum of $f$.
        But is the solution a global minimum?
        \begin{align}
            \Delta_1 &= H_f(x')_{1,1} = 2 > 0\\
            \Delta_2 &= det(H_f(x')) = 2 \cdot 2 - (-1 \cdot (-1)) = 4-1 = 3 > 0
        \end{align}
        All leading main minors of the $2 \times 2$ Hessian matrix $H_f(x')$ are greater than zero.\\
        $\Rightarrow$ $H_f(x')$ is positive definite!\\
        $\Rightarrow$ $x'$ is a local minimum!\\\\
        The degree of a polynomial indicates the number of possible zero crossings.
        Since the Jacobian matrix contains only polynomials with degree 1 as rows, there is also only one zero crossing at most, and thus also only one extremum at most.\\
        $\Rightarrow$ The local minimum $x'$ is also the global minimum!
        
    \subsection*{c)}
        \begin{align}
            f(x) &= 2x^3 -5x\\
            f'(x) &= 6x^2 -5\\
            f''(x) &= 12x
        \end{align}
        Let' consider $x=0$:
        \begin{align}
            x' = x - \frac{f'(x)}{f''(x)} = 0 - \frac{6 \cdot 0^2 -5}{12 \cdot 0} = ...\ \ \  \text{not possible! We can not divide by zero!}
        \end{align}
        No, obviously it is not always applicable if the function is twice continously differentiable!\\
        The problem is the starting point $x=0$.
        $x=0$ is the zero crossing of the second derivative $f''(x)$ of $f(x)$, and therefore an extremum of the first derivative $f'(x)$.
        Thus the tangent of $f'(x)$ at $x=0$ has no gradient, and therefore there is no intersection of this tangent with the x-axis!
        
    



\newpage
\section*{Exercise 5.5 - Convex Optimization}
$(0,5+0,5+0,5 = 1,5 \text{ points})$
    \subsection*{a)}
        \textbf{To show:}
        Let $f: \mathbib{C} \rightarrow \mathbb{R}$ be a convex function, defined on a convex $m$-dimensional set $\mathbib{C}$ in a real vector space and let $x \in \mathbb{R}^n$, $A \in \mathbb{R}^{m \times n}$ and $b \in \mathbb{R}^m$. Then we will proof that
        \begin{center}
            $f$ is convex $\Rightarrow$ $g(x) = f(Ax + b)$ is convex.
        \end{center}
        \begin{proof}
            For all $x_1$, $x_2 \in \mathbib{C}$ and $\lambda \in [0,1]$ it holds:
            \begin{align}
                g(\lambda x_1 + (1 - \lambda) x_2) &= f(\lambda (\underbrace{A x_1 + b}_{\in \mathbib{C}}) + (1 - \lambda) (\underbrace{A x_2 + b}_{\in \mathbib{C}}))\\
                &\leq \lambda f(A x_1 + b) + (1 - \lambda) f(A x_2 + b) \text{, \ \ \ \ \ \ because $f$ convex,}\\
                &= \lambda g(x_1) + (1 - \lambda) g(x_2)
            \end{align}
            Thus $g$ is a convex function!
        \end{proof}
        
        
        
    \subsection*{b)}
        \textbf{To show:} Every local minimum of a convex function is a global minimum.
        \begin{proof}
            Proof by contradiction:\\
            Let $f: \mathbib{C} \rightarrow \mathbb{R}$ be a convex function, defined on a convex set $\mathbib{C}$ in a real vector space, and assume $x^*$ is a local, but not a global minimum of $f$!\\
            Then there is at least one $y \in \mathbib{C}$ so that
            \begin{align}
                & & f(y) &< f(x^*)\\
                &\Leftrightarrow & f(y) - f(x^*) &< 0
            \end{align}
            By definition of convexity it holds for all $\lambda \in (0,1)$ that
            \begin{align}
                f(x^* + \lambda (y - x^*)) &\leq f(x^*) + \lambda f(y - x^*)\\
                &= f(x^*) + \underbrace{\underbrace{\lambda}_{0 < \lambda < 1} (\underbrace{f(y) - f(x^*)}_{< 0})}_{< 0}\\
                &< f(x^*)
            \end{align}
            Hence any small ball
            $$\mathcal{B}_{\epsilon}(x^*) := \{z \in \mathbib{C} : ||z-x^*|| \leq \epsilon\}$$
            with radius $\epsilon$ converging to zero is not empty, there exists at least one $z \in \mathcal{B}_{\epsilon}(x^*)$, so that
            $$f(z) < f(x^*)$$
            In other words, no matter how small the radius $\epsilon$, there is at least one $z \in \mathcal{B}_{\epsilon}(x^*)$ with $f(z) < f(x^*)$.\\
            But this is a contradiction to the given assumption that $x^*$ is a local minimum of $f$, because $x^*$ is only a local minimum, if there is an environment $\mathcal{B}_{\epsilon}(x^*) := \{z \in \mathbib{C} : ||z-x^*|| \leq \epsilon\} \neq \emptyset$ with $f(x^*) < f(z)$ for all $z \in \mathcal{B}_{\epsilon}(x^*)$.\\
            From this contradiction we can conclude that every local minimum $x^*$ of a convex function is also a global minimum!
        \end{proof}
        
        
    \subsection*{c)}
        Yes, the cross entropy loss function is convex, because we have the formula $L = - \sum\limits_{i=1}^n t_i log(p_i)$, for $n$ classes, where $t_i$ is the truth label and $p_i$ is the softmax probability for the ith class.\\
        If we always set one truth label $t_i$ equal to one, and all other equal to zero, we become $L_i = -log(p_i)$.
        If we do that for all $i \in \{1,...,n\}$ and compute the extrmum of all this $L_i$, we become the global minimum.
        The shape of them this function is very similar to a parable in higher dimensions, and therefore the function is convex, because all entries of the corresponding Hessian matrix are constants!
    
    





\end{document}
